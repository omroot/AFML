{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Sample Weights for Financial Machine Learning\n",
    "\n",
    "This notebook demonstrates the sample weighting techniques from **Advances in Financial Machine Learning** by Marcos López de Prado.\n",
    "\n",
    "## Why Sample Weights Matter\n",
    "\n",
    "Most machine learning algorithms assume that observations are **Independent and Identically Distributed (IID)**. This assumption is fundamentally violated in financial applications because:\n",
    "\n",
    "1. **Overlapping outcomes**: When using the triple-barrier method, labels can depend on overlapping time periods\n",
    "2. **Serial correlation**: Financial returns exhibit autocorrelation\n",
    "3. **Regime changes**: Market conditions evolve over time, making older observations less relevant\n",
    "\n",
    "This chapter addresses these issues through:\n",
    "- **Uniqueness estimation**: Measuring how much independent information each sample contains\n",
    "- **Sequential bootstrap**: Sampling method that accounts for overlaps\n",
    "- **Return attribution**: Weighting samples by their informational content\n",
    "- **Time decay**: Reducing the influence of older observations\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [The Problem: Overlapping Outcomes](#1.-The-Problem:-Overlapping-Outcomes)\n",
    "2. [Concurrent Labels and Uniqueness](#2.-Concurrent-Labels-and-Uniqueness)\n",
    "3. [The Sequential Bootstrap](#3.-The-Sequential-Bootstrap)\n",
    "4. [Sample Weights by Return Attribution](#4.-Sample-Weights-by-Return-Attribution)\n",
    "5. [Time Decay](#5.-Time-Decay)\n",
    "6. [Putting It All Together](#6.-Putting-It-All-Together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent))\n",
    "\n",
    "# Import our modules\n",
    "from afml.labeling import (\n",
    "    get_daily_volatility,\n",
    "    get_events,\n",
    "    add_vertical_barrier,\n",
    "    get_labels,\n",
    ")\n",
    "from afml.sample_weights import (\n",
    "    get_num_concurrent_events,\n",
    "    get_average_uniqueness,\n",
    "    compute_sample_uniqueness,\n",
    "    get_indicator_matrix,\n",
    "    get_average_uniqueness_from_matrix,\n",
    "    sequential_bootstrap,\n",
    "    compare_bootstrap_methods,\n",
    "    get_sample_weights_by_return,\n",
    "    compute_sample_weights,\n",
    "    apply_time_decay,\n",
    "    apply_exponential_decay,\n",
    ")\n",
    "\n",
    "# Plotting settings\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "    except:\n",
    "        pass\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data\n",
    "\n",
    "We'll use synthetic price data to demonstrate the concepts. This allows us to control the data properties and clearly illustrate the effects of different weighting schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_prices(\n",
    "    n_days: int = 250,\n",
    "    initial_price: float = 100.0,\n",
    "    annual_volatility: float = 0.20,\n",
    "    freq: str = 'h',\n",
    ") -> pd.Series:\n",
    "    \"\"\"Generate synthetic price data using geometric Brownian motion.\"\"\"\n",
    "    periods = n_days * 24 if freq == 'h' else n_days\n",
    "    index = pd.date_range(start='2020-01-01', periods=periods, freq=freq)\n",
    "    \n",
    "    dt = 1 / 252 if freq == 'D' else 1 / (252 * 24)\n",
    "    daily_vol = annual_volatility * np.sqrt(dt)\n",
    "    \n",
    "    returns = daily_vol * np.random.randn(periods)\n",
    "    prices = initial_price * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    return pd.Series(prices, index=index, name='close')\n",
    "\n",
    "\n",
    "# Generate price data\n",
    "close_prices = generate_synthetic_prices(n_days=250)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "close_prices.plot(ax=ax, linewidth=0.8)\n",
    "ax.set_title('Synthetic Price Series', fontsize=14)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Price data: {len(close_prices)} bars\")\n",
    "print(f\"Date range: {close_prices.index[0]} to {close_prices.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Problem: Overlapping Outcomes\n",
    "\n",
    "### Why IID Matters\n",
    "\n",
    "Most ML algorithms assume that training samples are **independent** - knowing one sample tells you nothing about another. In finance, this assumption is violated when we use the triple-barrier method.\n",
    "\n",
    "### The Blood Sample Analogy\n",
    "\n",
    "Imagine you're a medical researcher with blood samples from 100 patients. You want to predict cholesterol levels from diet and exercise data.\n",
    "\n",
    "**Normal case (IID)**: Each tube contains blood from exactly one patient. The samples are independent.\n",
    "\n",
    "**Financial ML case (non-IID)**: Someone accidentally spills blood from each tube into the next 9 tubes. Now:\n",
    "- Tube 10 contains blood from patients 1-10\n",
    "- Tube 11 contains blood from patients 2-11\n",
    "- And so on...\n",
    "\n",
    "The \"spillage\" in financial ML comes from **overlapping holding periods**.\n",
    "\n",
    "### Visualizing the Overlap\n",
    "\n",
    "Let's create events using the triple-barrier method and visualize how they overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create events using triple-barrier method\n",
    "sample_indices = close_prices.index[::24]  # Sample daily\n",
    "timestamp_events = pd.DatetimeIndex(sample_indices[:-20])\n",
    "\n",
    "# Compute volatility for barrier thresholds\n",
    "target_returns = get_daily_volatility(close_prices, lookback_span=50)\n",
    "\n",
    "# Add vertical barriers (5-day holding period)\n",
    "vertical_barriers = add_vertical_barrier(\n",
    "    timestamp_events=timestamp_events,\n",
    "    close_prices=close_prices,\n",
    "    num_days=5,\n",
    ")\n",
    "\n",
    "# Get events\n",
    "events = get_events(\n",
    "    close_prices=close_prices,\n",
    "    timestamp_events=timestamp_events,\n",
    "    profit_taking_stop_loss=[2.0, 2.0],\n",
    "    target_returns=target_returns,\n",
    "    min_return=0.0001,\n",
    "    num_threads=1,\n",
    "    vertical_barrier_times=vertical_barriers,\n",
    "    side=None,\n",
    ")\n",
    "\n",
    "print(f\"Number of events: {len(events)}\")\n",
    "print(f\"\\nSample events (showing start time and end time t1):\")\n",
    "print(events.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_event_overlaps(events_df, num_events=15):\n",
    "    \"\"\"\n",
    "    Visualize how events overlap in time.\n",
    "    Each horizontal bar represents an event's lifespan.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    subset = events_df.head(num_events)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, num_events))\n",
    "    \n",
    "    for i, (start_time, row) in enumerate(subset.iterrows()):\n",
    "        end_time = row['t1']\n",
    "        if pd.notna(end_time):\n",
    "            ax.barh(\n",
    "                y=i,\n",
    "                width=(end_time - start_time).total_seconds() / 3600,\n",
    "                left=(start_time - subset.index[0]).total_seconds() / 3600,\n",
    "                height=0.8,\n",
    "                color=colors[i],\n",
    "                alpha=0.7,\n",
    "                edgecolor='black',\n",
    "                linewidth=0.5,\n",
    "            )\n",
    "    \n",
    "    ax.set_xlabel('Hours from first event', fontsize=12)\n",
    "    ax.set_ylabel('Event Index', fontsize=12)\n",
    "    ax.set_title('Event Lifespans: Visualizing Overlapping Outcomes', fontsize=14)\n",
    "    ax.set_yticks(range(num_events))\n",
    "    \n",
    "    # Add grid lines\n",
    "    ax.grid(True, axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_event_overlaps(events, num_events=15)\n",
    "\n",
    "print(\"\"\"\\n\n",
    "OBSERVATION: Notice how events overlap significantly.\n",
    "- Event 0 might overlap with Events 1, 2, 3, and 4\n",
    "- This means their labels are NOT independent\n",
    "- Standard ML methods will treat them as if they were independent\n",
    "- This leads to overfitting and inflated accuracy estimates\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Consequences of Ignoring Overlap\n",
    "\n",
    "When we ignore overlapping outcomes:\n",
    "\n",
    "1. **Redundant information**: The same market move affects multiple labels\n",
    "2. **Overfitting**: The model memorizes the redundant patterns\n",
    "3. **Inflated metrics**: Out-of-bag and cross-validation scores are too optimistic\n",
    "4. **Poor generalization**: The model fails on truly new data\n",
    "\n",
    "The solution is to **measure and account for the overlap** using uniqueness scores and sample weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Concurrent Labels and Uniqueness\n",
    "\n",
    "### Concurrency: Counting Overlaps\n",
    "\n",
    "Two labels $y_i$ and $y_j$ are **concurrent** at time $t$ when both depend on the return at time $t$.\n",
    "\n",
    "For each time point, we count how many labels are \"active\" (their event spans that time):\n",
    "\n",
    "$$c_t = \\sum_{i=1}^{I} \\mathbb{1}_{t,i}$$\n",
    "\n",
    "where $\\mathbb{1}_{t,i} = 1$ if event $i$ spans time $t$, and 0 otherwise.\n",
    "\n",
    "### Uniqueness: Measuring Independence\n",
    "\n",
    "The **uniqueness** of label $i$ at time $t$ is:\n",
    "\n",
    "$$u_{t,i} = \\frac{1}{c_t}$$\n",
    "\n",
    "The **average uniqueness** of label $i$ over its lifespan is:\n",
    "\n",
    "$$\\bar{u}_i = \\frac{\\sum_{t=t_0}^{t_1} u_{t,i}}{\\sum_{t=t_0}^{t_1} \\mathbb{1}_{t,i}}$$\n",
    "\n",
    "**Interpretation:**\n",
    "- $\\bar{u}_i = 1$: Label $i$ has no overlap with any other label (fully unique)\n",
    "- $\\bar{u}_i = 0.5$: On average, label $i$ shares its information with one other label\n",
    "- $\\bar{u}_i \\to 0$: Label $i$ is highly redundant with many other labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute concurrent events at each time point\n",
    "event_end_times = events['t1']\n",
    "\n",
    "num_concurrent = get_num_concurrent_events(\n",
    "    close_index=close_prices.index,\n",
    "    event_end_times=event_end_times,\n",
    ")\n",
    "\n",
    "# Plot concurrency over time\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# Price\n",
    "price_subset = close_prices.loc[num_concurrent.index[0]:num_concurrent.index[-1]]\n",
    "axes[0].plot(price_subset.index, price_subset.values, linewidth=0.8)\n",
    "axes[0].set_title('Price Series', fontsize=12)\n",
    "axes[0].set_ylabel('Price')\n",
    "\n",
    "# Concurrency\n",
    "axes[1].fill_between(num_concurrent.index, 0, num_concurrent.values, alpha=0.7)\n",
    "axes[1].set_title('Number of Concurrent Labels at Each Time Point', fontsize=12)\n",
    "axes[1].set_ylabel('Concurrent Labels')\n",
    "axes[1].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Concurrency statistics:\")\n",
    "print(f\"  Mean: {num_concurrent.mean():.2f}\")\n",
    "print(f\"  Max:  {num_concurrent.max():.0f}\")\n",
    "print(f\"  Min:  {num_concurrent.min():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average uniqueness for each event\n",
    "# Handle duplicates and reindex\n",
    "num_concurrent_clean = num_concurrent.loc[~num_concurrent.index.duplicated(keep='last')]\n",
    "num_concurrent_reindexed = num_concurrent_clean.reindex(close_prices.index).fillna(1)\n",
    "\n",
    "avg_uniqueness = get_average_uniqueness(\n",
    "    event_end_times=event_end_times,\n",
    "    num_concurrent_events=num_concurrent_reindexed,\n",
    ")\n",
    "\n",
    "# Plot histogram of uniqueness\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(avg_uniqueness.dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=avg_uniqueness.mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {avg_uniqueness.mean():.3f}')\n",
    "axes[0].set_title('Distribution of Average Uniqueness', fontsize=12)\n",
    "axes[0].set_xlabel('Average Uniqueness')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "# Time series\n",
    "axes[1].plot(avg_uniqueness.index, avg_uniqueness.values, linewidth=0.8)\n",
    "axes[1].axhline(y=avg_uniqueness.mean(), color='red', linestyle='--', alpha=0.7)\n",
    "axes[1].set_title('Average Uniqueness Over Time', fontsize=12)\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Average Uniqueness')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage uniqueness statistics:\")\n",
    "print(f\"  Mean: {avg_uniqueness.mean():.4f}\")\n",
    "print(f\"  Std:  {avg_uniqueness.std():.4f}\")\n",
    "print(f\"  Min:  {avg_uniqueness.min():.4f}\")\n",
    "print(f\"  Max:  {avg_uniqueness.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Uniqueness\n",
    "\n",
    "The histogram above shows how uniqueness is distributed across our events:\n",
    "\n",
    "- **Peak location**: Where most events fall in terms of uniqueness\n",
    "- **Spread**: How much variation exists in overlap patterns\n",
    "- **Low values**: Events that are highly redundant (share information with many others)\n",
    "\n",
    "**Key insight**: If average uniqueness is low (e.g., 0.2), it means each observation effectively represents only 20% of a truly independent sample. Standard bootstrap would oversample by a factor of 5!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. The Sequential Bootstrap\n",
    "\n",
    "### The Problem with Standard Bootstrap\n",
    "\n",
    "Standard bootstrap assumes IID observations and samples with uniform probability. When observations overlap:\n",
    "\n",
    "1. **Redundant draws**: High probability of drawing overlapping samples\n",
    "2. **Oversampling**: The same information is represented multiple times\n",
    "3. **Overfitting**: Random forests become collections of similar overfit trees\n",
    "4. **Inflated OOB scores**: Out-of-bag samples are too similar to in-bag samples\n",
    "\n",
    "### The Sequential Bootstrap Solution\n",
    "\n",
    "Sequential bootstrap adjusts sampling probabilities to reduce overlap:\n",
    "\n",
    "1. **First draw**: Uniform probability (all events equally likely)\n",
    "2. **Subsequent draws**: Probability proportional to uniqueness given already-selected events\n",
    "3. **Effect**: Events overlapping with selected ones become less likely to be drawn\n",
    "\n",
    "### The Indicator Matrix\n",
    "\n",
    "The foundation of sequential bootstrap is the **indicator matrix** $\\{\\mathbb{1}_{t,i}\\}$:\n",
    "\n",
    "- Rows: Time points (bars)\n",
    "- Columns: Events\n",
    "- Value: 1 if event spans that time, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example from the book (Section 4.5.3)\n",
    "# y1: r[0,3], y2: r[2,4], y3: r[4,6]\n",
    "\n",
    "example_bar_idx = pd.RangeIndex(7)  # bars 0-6\n",
    "example_t1 = pd.Series([2, 3, 5], index=[0, 2, 4])\n",
    "\n",
    "# Build indicator matrix\n",
    "example_ind_matrix = get_indicator_matrix(example_bar_idx, example_t1)\n",
    "\n",
    "print(\"Indicator Matrix (Book Example):\")\n",
    "print(\"================================\")\n",
    "print(\"Rows = time points (bars 0-6)\")\n",
    "print(\"Columns = events (0, 1, 2)\")\n",
    "print()\n",
    "print(example_ind_matrix)\n",
    "print()\n",
    "print(\"Event spans:\")\n",
    "print(\"  Event 0: bars 0-2 (depends on returns r[0,1], r[1,2])\")\n",
    "print(\"  Event 1: bars 2-3 (depends on returns r[2,3])\")\n",
    "print(\"  Event 2: bars 4-5 (depends on returns r[4,5])\")\n",
    "print()\n",
    "print(\"Overlaps:\")\n",
    "print(\"  Events 0 and 1 overlap at bar 2\")\n",
    "print(\"  Event 2 has no overlap with others\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the indicator matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "im = ax.imshow(example_ind_matrix.values, cmap='Blues', aspect='auto')\n",
    "ax.set_xticks(range(3))\n",
    "ax.set_xticklabels(['Event 0', 'Event 1', 'Event 2'])\n",
    "ax.set_yticks(range(7))\n",
    "ax.set_yticklabels([f'Bar {i}' for i in range(7)])\n",
    "ax.set_title('Indicator Matrix Visualization', fontsize=14)\n",
    "ax.set_xlabel('Events')\n",
    "ax.set_ylabel('Time (Bars)')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(7):\n",
    "    for j in range(3):\n",
    "        text = ax.text(j, i, example_ind_matrix.iloc[i, j],\n",
    "                       ha='center', va='center', fontsize=12,\n",
    "                       color='white' if example_ind_matrix.iloc[i, j] else 'gray')\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Active (1) / Inactive (0)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average uniqueness from indicator matrix\n",
    "example_uniqueness = get_average_uniqueness_from_matrix(example_ind_matrix)\n",
    "\n",
    "print(\"Average Uniqueness:\")\n",
    "print(\"==================\")\n",
    "for i, uniq in enumerate(example_uniqueness):\n",
    "    print(f\"  Event {i}: {uniq:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"  Event 0: 0.833 - spans 3 bars, 1 bar overlaps with Event 1\")\n",
    "print(\"  Event 1: 0.750 - spans 2 bars, 1 bar overlaps with Event 0\") \n",
    "print(\"  Event 2: 1.000 - no overlap with any other event (fully unique)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Bootstrap: Step by Step\n",
    "\n",
    "Let's trace through the sequential bootstrap algorithm:\n",
    "\n",
    "**Draw 1**: All events have equal probability (1/3 each)\n",
    "- Suppose we draw Event 2\n",
    "\n",
    "**Draw 2**: Update probabilities based on overlap with {Event 2}\n",
    "- Event 0: No overlap with Event 2 → High probability\n",
    "- Event 1: No overlap with Event 2 → High probability  \n",
    "- Event 2: Perfect overlap with itself → Lower probability\n",
    "\n",
    "**Draw 3**: Update probabilities based on overlap with drawn events\n",
    "- Probabilities adjust to favor less overlapping events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sequential bootstrap multiple times\n",
    "print(\"Sequential Bootstrap Samples (10 runs):\")\n",
    "print(\"=======================================\")\n",
    "for i in range(10):\n",
    "    sample = sequential_bootstrap(example_ind_matrix, random_state=i)\n",
    "    sample_uniqueness = get_average_uniqueness_from_matrix(\n",
    "        example_ind_matrix.iloc[:, sample]\n",
    "    ).mean()\n",
    "    print(f\"  Run {i}: {sample} - Avg uniqueness: {sample_uniqueness:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For large real-world datasets, building the full indicator matrix can be memory-intensive\n",
    "# Here's how you would do it (commented out for performance):\n",
    "#\n",
    "# real_ind_matrix = get_indicator_matrix(\n",
    "#     bar_index=close_prices.index,\n",
    "#     event_end_times=event_end_times,\n",
    "# )\n",
    "\n",
    "# For our demonstration, we'll use the small example matrix\n",
    "# which clearly shows the sequential bootstrap advantage\n",
    "print(\"For real-world applications:\")\n",
    "print(f\"  - You would have ~{len(close_prices)} bars x ~{len(events)} events\")\n",
    "print(f\"  - That's a {len(close_prices)} x {len(events)} indicator matrix\")\n",
    "print(\"  - Sequential bootstrap can be computationally expensive\")\n",
    "print(\"  - Consider using the max_samples approach instead for large datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare standard vs sequential bootstrap\n",
    "# Note: Sequential bootstrap is computationally expensive on large datasets\n",
    "# We'll use the small example for demonstration\n",
    "\n",
    "# Compare bootstrap methods on the small example\n",
    "comparison = compare_bootstrap_methods(\n",
    "    indicator_matrix=example_ind_matrix,\n",
    "    num_iterations=100,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histograms\n",
    "axes[0].hist(comparison['standard_uniqueness'], bins=15, alpha=0.7, \n",
    "             label='Standard Bootstrap', edgecolor='black')\n",
    "axes[0].hist(comparison['sequential_uniqueness'], bins=15, alpha=0.7,\n",
    "             label='Sequential Bootstrap', edgecolor='black')\n",
    "axes[0].axvline(x=comparison['standard_uniqueness'].median(), color='blue',\n",
    "                linestyle='--', label=f'Std Median: {comparison[\"standard_uniqueness\"].median():.3f}')\n",
    "axes[0].axvline(x=comparison['sequential_uniqueness'].median(), color='orange',\n",
    "                linestyle='--', label=f'Seq Median: {comparison[\"sequential_uniqueness\"].median():.3f}')\n",
    "axes[0].set_title('Bootstrap Sample Uniqueness Comparison', fontsize=12)\n",
    "axes[0].set_xlabel('Average Uniqueness')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot([comparison['standard_uniqueness'], comparison['sequential_uniqueness']],\n",
    "                labels=['Standard', 'Sequential'])\n",
    "axes[1].set_title('Uniqueness Distribution by Method', fontsize=12)\n",
    "axes[1].set_ylabel('Average Uniqueness')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStatistical Comparison:\")\n",
    "print(comparison.describe())\n",
    "print(f\"\\nImprovement: Sequential bootstrap achieves {(comparison['sequential_uniqueness'].mean() - comparison['standard_uniqueness'].mean()) / comparison['standard_uniqueness'].mean() * 100:.1f}% higher average uniqueness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway\n",
    "\n",
    "Sequential bootstrap produces samples with **higher average uniqueness** than standard bootstrap. This means:\n",
    "\n",
    "1. Less redundancy in training data\n",
    "2. More diverse decision trees in random forests\n",
    "3. More realistic out-of-bag error estimates\n",
    "4. Better generalization to new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Sample Weights by Return Attribution\n",
    "\n",
    "### The Weighting Problem\n",
    "\n",
    "Even with sequential bootstrap, we need to tell the ML algorithm how much to \"trust\" each sample. Two factors matter:\n",
    "\n",
    "1. **Uniqueness**: How much independent information does this sample contain?\n",
    "2. **Return magnitude**: How significant is the price move that determined this label?\n",
    "\n",
    "### Return Attribution\n",
    "\n",
    "The weight for event $i$ is the absolute sum of attributed returns over its lifespan:\n",
    "\n",
    "$$\\tilde{w}_i = \\left| \\sum_{t=t_{i,0}}^{t_{i,1}} \\frac{r_{t-1,t}}{c_t} \\right|$$\n",
    "\n",
    "where:\n",
    "- $r_{t-1,t}$ is the log return at time $t$\n",
    "- $c_t$ is the concurrency at time $t$\n",
    "\n",
    "**Intuition**: \n",
    "- Divide each return by concurrency (attribute fairly across overlapping events)\n",
    "- Sum the attributed returns over the event's lifespan\n",
    "- Take absolute value (we care about magnitude, not direction)\n",
    "\n",
    "Finally, scale weights to sum to the number of observations:\n",
    "\n",
    "$$w_i = \\tilde{w}_i \\cdot \\frac{I}{\\sum_{j=1}^{I} \\tilde{w}_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sample weights by return attribution\n",
    "sample_weights = compute_sample_weights(\n",
    "    event_end_times=event_end_times,\n",
    "    close_prices=close_prices,\n",
    "    use_returns=True,\n",
    ")\n",
    "\n",
    "print(f\"Sample weights computed for {len(sample_weights)} events\")\n",
    "print(f\"\\nWeight statistics:\")\n",
    "print(f\"  Sum: {sample_weights.sum():.2f} (should equal {len(sample_weights)})\")\n",
    "print(f\"  Mean: {sample_weights.mean():.4f}\")\n",
    "print(f\"  Std: {sample_weights.std():.4f}\")\n",
    "print(f\"  Min: {sample_weights.min():.4f}\")\n",
    "print(f\"  Max: {sample_weights.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample weights\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Histogram of weights\n",
    "axes[0, 0].hist(sample_weights, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(x=1.0, color='red', linestyle='--', label='Default weight (1.0)')\n",
    "axes[0, 0].set_title('Distribution of Sample Weights', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Weight')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Weights over time\n",
    "axes[0, 1].plot(sample_weights.index, sample_weights.values, linewidth=0.8)\n",
    "axes[0, 1].axhline(y=1.0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[0, 1].set_title('Sample Weights Over Time', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Weight')\n",
    "\n",
    "# Weight vs Uniqueness\n",
    "common_idx = sample_weights.index.intersection(avg_uniqueness.index)\n",
    "axes[1, 0].scatter(avg_uniqueness.loc[common_idx], sample_weights.loc[common_idx], \n",
    "                   alpha=0.5, s=20)\n",
    "axes[1, 0].set_title('Weight vs Uniqueness', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Average Uniqueness')\n",
    "axes[1, 0].set_ylabel('Sample Weight')\n",
    "\n",
    "# Get labels and show weight vs return\n",
    "labels = get_labels(events, close_prices)\n",
    "common_idx_labels = sample_weights.index.intersection(labels.index)\n",
    "axes[1, 1].scatter(labels.loc[common_idx_labels, 'ret'].abs(), \n",
    "                   sample_weights.loc[common_idx_labels], alpha=0.5, s=20)\n",
    "axes[1, 1].set_title('Weight vs Absolute Return', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Absolute Return')\n",
    "axes[1, 1].set_ylabel('Sample Weight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Weights\n",
    "\n",
    "The scatter plots reveal important patterns:\n",
    "\n",
    "1. **Weight vs Uniqueness**: Higher uniqueness generally leads to higher weights (more independent information)\n",
    "\n",
    "2. **Weight vs Return**: Larger absolute returns lead to higher weights (more significant price moves)\n",
    "\n",
    "3. **Combined effect**: A sample with high uniqueness AND large return gets the highest weight\n",
    "\n",
    "**Usage in sklearn**:\n",
    "```python\n",
    "model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Time Decay\n",
    "\n",
    "### Why Time Decay?\n",
    "\n",
    "Markets are **adaptive systems** (Lo, 2017). As market participants learn and adapt:\n",
    "\n",
    "- Old patterns become less relevant\n",
    "- Recent observations are more predictive of future behavior\n",
    "- Models trained on stale data may fail\n",
    "\n",
    "Time decay reduces the weight of older observations, helping the model focus on recent patterns.\n",
    "\n",
    "### The Decay Function\n",
    "\n",
    "We apply a **piecewise-linear decay** function:\n",
    "\n",
    "$$d(x) = \\max\\{0, a + bx\\}$$\n",
    "\n",
    "where $x$ is cumulative uniqueness (not chronological time!).\n",
    "\n",
    "The parameter $c \\in (-1, 1]$ controls the decay shape:\n",
    "\n",
    "| Parameter $c$ | Effect |\n",
    "|--------------|--------|\n",
    "| $c = 1$ | No decay (all weights unchanged) |\n",
    "| $0 < c < 1$ | Linear decay; oldest gets weight $c$ |\n",
    "| $c = 0$ | Linear decay to zero for oldest |\n",
    "| $-1 < c < 0$ | Oldest $|c|$ fraction gets zero weight |\n",
    "\n",
    "**Why cumulative uniqueness instead of time?**\n",
    "\n",
    "If we decay by chronological time, redundant observations would be penalized too quickly. Using cumulative uniqueness ensures that decay is proportional to actual information content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different decay factors\n",
    "decay_factors = [1.0, 0.75, 0.5, 0.0, -0.25, -0.5]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, c in enumerate(decay_factors):\n",
    "    decayed = apply_time_decay(sample_weights, decay_factor=c)\n",
    "    \n",
    "    # Plot original vs decayed\n",
    "    axes[i].plot(sample_weights.index, sample_weights.values, \n",
    "                 alpha=0.5, label='Original', linewidth=0.8)\n",
    "    axes[i].plot(decayed.index, decayed.values, \n",
    "                 label='Decayed', linewidth=0.8)\n",
    "    axes[i].set_title(f'Decay Factor c = {c}', fontsize=12)\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].set_ylabel('Weight')\n",
    "    axes[i].legend()\n",
    "    \n",
    "    # Add annotation\n",
    "    if c == 1.0:\n",
    "        axes[i].annotate('No decay', xy=(0.5, 0.9), xycoords='axes fraction',\n",
    "                         fontsize=10, ha='center')\n",
    "    elif c == 0:\n",
    "        axes[i].annotate('Oldest → 0', xy=(0.5, 0.9), xycoords='axes fraction',\n",
    "                         fontsize=10, ha='center')\n",
    "    elif c < 0:\n",
    "        axes[i].annotate(f'Oldest {abs(c)*100:.0f}% = 0', xy=(0.5, 0.9), \n",
    "                         xycoords='axes fraction', fontsize=10, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare linear vs exponential decay\n",
    "linear_decayed = apply_time_decay(sample_weights, decay_factor=0.5)\n",
    "exp_decayed = apply_exponential_decay(sample_weights, half_life=sample_weights.sum() / 4)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(sample_weights.index, sample_weights.values, \n",
    "        alpha=0.4, label='Original', linewidth=0.8)\n",
    "ax.plot(linear_decayed.index, linear_decayed.values,\n",
    "        label='Linear Decay (c=0.5)', linewidth=1)\n",
    "ax.plot(exp_decayed.index, exp_decayed.values,\n",
    "        label='Exponential Decay', linewidth=1, linestyle='--')\n",
    "\n",
    "ax.set_title('Linear vs Exponential Time Decay', fontsize=14)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Weight')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Comparison of decay methods:\")\n",
    "print(f\"  Original sum: {sample_weights.sum():.2f}\")\n",
    "print(f\"  Linear decayed sum: {linear_decayed.sum():.2f}\")\n",
    "print(f\"  Exponential decayed sum: {exp_decayed.sum():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Decay Factor\n",
    "\n",
    "The optimal decay factor depends on:\n",
    "\n",
    "1. **Market regime stability**: Rapidly changing markets → stronger decay (lower $c$)\n",
    "2. **Data volume**: More data → can afford to discard oldest observations\n",
    "3. **Prediction horizon**: Short-term predictions → stronger decay\n",
    "4. **Strategy type**: Mean-reversion may need recent data; momentum may benefit from longer history\n",
    "\n",
    "**Recommendation**: Start with $c = 0.5$ and tune based on out-of-sample performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Putting It All Together\n",
    "\n",
    "### The Complete Sample Weighting Pipeline\n",
    "\n",
    "Here's how to combine all the techniques for training an ML model:\n",
    "\n",
    "```python\n",
    "# 1. Generate labels using triple-barrier method\n",
    "events = get_events(...)\n",
    "labels = get_labels(events, close_prices)\n",
    "\n",
    "# 2. Compute sample weights\n",
    "sample_weights = compute_sample_weights(\n",
    "    event_end_times=events['t1'],\n",
    "    close_prices=close_prices,\n",
    "    use_returns=True,\n",
    ")\n",
    "\n",
    "# 3. Apply time decay\n",
    "decayed_weights = apply_time_decay(sample_weights, decay_factor=0.5)\n",
    "\n",
    "# 4. Train model with weights\n",
    "model.fit(X_train, y_train, sample_weight=decayed_weights)\n",
    "\n",
    "# 5. For bagging/random forests, also use sequential bootstrap\n",
    "# Set max_samples based on average uniqueness\n",
    "avg_uniqueness = decayed_weights.mean() / len(decayed_weights)\n",
    "bagging_clf = BaggingClassifier(\n",
    "    max_samples=avg_uniqueness,\n",
    "    ...\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete example\n",
    "print(\"Complete Sample Weighting Pipeline\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Step 1: We already have events and labels\n",
    "print(f\"\\n1. Events: {len(events)} samples\")\n",
    "print(f\"   Labels: {len(labels)} samples\")\n",
    "\n",
    "# Step 2: Compute sample weights\n",
    "weights = compute_sample_weights(\n",
    "    event_end_times=events['t1'],\n",
    "    close_prices=close_prices,\n",
    "    use_returns=True,\n",
    ")\n",
    "print(f\"\\n2. Sample weights computed\")\n",
    "print(f\"   Mean: {weights.mean():.4f}\")\n",
    "print(f\"   Std: {weights.std():.4f}\")\n",
    "\n",
    "# Step 3: Apply time decay\n",
    "final_weights = apply_time_decay(weights, decay_factor=0.5)\n",
    "print(f\"\\n3. Time decay applied (c=0.5)\")\n",
    "print(f\"   Mean: {final_weights.mean():.4f}\")\n",
    "print(f\"   Std: {final_weights.std():.4f}\")\n",
    "\n",
    "# Step 4: Compute uniqueness for max_samples\n",
    "uniqueness_df = compute_sample_uniqueness(close_prices, events['t1'])\n",
    "avg_uniq = uniqueness_df['average_uniqueness'].mean()\n",
    "print(f\"\\n4. Average uniqueness: {avg_uniq:.4f}\")\n",
    "print(f\"   Recommended max_samples for bagging: {avg_uniq:.4f}\")\n",
    "\n",
    "# Summary visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(final_weights, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Final Sample Weights', fontsize=12)\n",
    "axes[0].set_xlabel('Weight')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].plot(final_weights.index, final_weights.values, linewidth=0.8)\n",
    "axes[1].set_title('Weights Over Time', fontsize=12)\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Weight')\n",
    "\n",
    "# Show weight by label\n",
    "common_idx = final_weights.index.intersection(labels.index)\n",
    "for label_val in sorted(labels['bin'].unique()):\n",
    "    mask = labels.loc[common_idx, 'bin'] == label_val\n",
    "    label_weights = final_weights.loc[common_idx][mask]\n",
    "    axes[2].hist(label_weights, bins=20, alpha=0.6, \n",
    "                 label=f'Label {int(label_val)}')\n",
    "axes[2].set_title('Weights by Label', fontsize=12)\n",
    "axes[2].set_xlabel('Weight')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Financial data violates IID assumptions**: Overlapping outcomes create dependencies between labels\n",
    "\n",
    "2. **Uniqueness measures independence**: The average uniqueness score tells us how much independent information each sample contains\n",
    "\n",
    "3. **Sequential bootstrap reduces redundancy**: By adjusting sampling probabilities, we get samples closer to IID\n",
    "\n",
    "4. **Return attribution weights samples**: Combines uniqueness with return magnitude to determine sample importance\n",
    "\n",
    "5. **Time decay focuses on recent data**: Markets evolve, so recent observations are more relevant\n",
    "\n",
    "### Function Reference\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|--------|\n",
    "| `get_num_concurrent_events()` | Count overlapping labels at each time |\n",
    "| `get_average_uniqueness()` | Compute uniqueness score for each label |\n",
    "| `get_indicator_matrix()` | Build binary matrix for bootstrap |\n",
    "| `sequential_bootstrap()` | Sample with overlap-aware probabilities |\n",
    "| `compute_sample_weights()` | Compute weights by return attribution |\n",
    "| `apply_time_decay()` | Apply linear time decay to weights |\n",
    "| `apply_exponential_decay()` | Apply exponential time decay |\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "1. **Always compute sample weights** when using triple-barrier labels\n",
    "2. **Use sequential bootstrap** for bagging classifiers (Random Forest, etc.)\n",
    "3. **Set `max_samples`** to average uniqueness for sklearn's BaggingClassifier\n",
    "4. **Apply time decay** with $c \\approx 0.5$ as a starting point\n",
    "5. **Use `class_weight='balanced'`** in sklearn to handle label imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Vary the holding period**: Change the vertical barrier from 5 days to 10 days. How does this affect average uniqueness?\n",
    "\n",
    "2. **Monte Carlo comparison**: Run `compare_bootstrap_methods()` with 1000 iterations. Is the difference between standard and sequential bootstrap statistically significant?\n",
    "\n",
    "3. **Time decay sensitivity**: Try different decay factors ($c = 0.25, 0.5, 0.75$) and observe how weight distributions change.\n",
    "\n",
    "4. **Implement exponential decay**: The book suggests exponential decay as an exercise. Compare its behavior to linear decay on your data.\n",
    "\n",
    "5. **Cross-validation impact**: Train a simple classifier with and without sample weights. Compare cross-validation scores to see the effect of proper weighting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
